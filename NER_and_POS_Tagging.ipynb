{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_and_POS_Tagging.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Stanza for English NER"
      ],
      "metadata": {
        "id": "7rZowIGC4SfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEuz4PwkTFlh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33aa42f9-9012-4deb-c277-60210e281b0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.3.0-py3-none-any.whl (432 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 19.7 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 61 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 184 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 235 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 245 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 307 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 358 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 368 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 419 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 430 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 432 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.10.0+cu111)\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 28.4 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 35.9 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 42.5 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 46.5 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61 kB 51.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 54.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 56.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 92 kB 60.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 102 kB 62.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 112 kB 62.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 122 kB 62.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 133 kB 62.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 143 kB 62.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 153 kB 62.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 163 kB 62.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 174 kB 62.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 175 kB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from stanza) (1.15.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=70b44657f1e6cfd2fc4eeb0be90dddd533d0171c070366d32a4e5fc4a549e958\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, stanza\n",
            "Successfully installed emoji-1.7.0 stanza-1.3.0\n",
            "Collecting stanza-batch\n",
            "  Downloading stanza_batch-0.2.2-py3-none-any.whl (14 kB)\n",
            "Collecting stanza<=1.2.0,>=1.1.1\n",
            "  Downloading stanza-1.2-py3-none-any.whl (282 kB)\n",
            "\u001b[K     |████████████████████████████████| 282 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza<=1.2.0,>=1.1.1->stanza-batch) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza<=1.2.0,>=1.1.1->stanza-batch) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza<=1.2.0,>=1.1.1->stanza-batch) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza<=1.2.0,>=1.1.1->stanza-batch) (1.10.0+cu111)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza<=1.2.0,>=1.1.1->stanza-batch) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza<=1.2.0,>=1.1.1->stanza-batch) (4.1.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza<=1.2.0,>=1.1.1->stanza-batch) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza<=1.2.0,>=1.1.1->stanza-batch) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza<=1.2.0,>=1.1.1->stanza-batch) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza<=1.2.0,>=1.1.1->stanza-batch) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza<=1.2.0,>=1.1.1->stanza-batch) (2.10)\n",
            "Installing collected packages: stanza, stanza-batch\n",
            "  Attempting uninstall: stanza\n",
            "    Found existing installation: stanza 1.3.0\n",
            "    Uninstalling stanza-1.3.0:\n",
            "      Successfully uninstalled stanza-1.3.0\n",
            "Successfully installed stanza-1.2 stanza-batch-0.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install stanza\n",
        "!pip install stanza-batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "import pandas as pd\n",
        "from stanza.models.common.doc import Document\n",
        "from typing import List\n",
        "from stanza_batch import batch\n",
        "stanza.download(lang='en', processors='tokenize,ner')\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VDrhHqs3igj",
        "outputId": "6fedaed7-62cb-416b-9f4f-f697ed0fae13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 28.6MB/s]                    \n",
            "2022-04-22 07:30:09 INFO: Downloading these customized packages for language: en (English)...\n",
            "===============================\n",
            "| Processor       | Package   |\n",
            "-------------------------------\n",
            "| tokenize        | combined  |\n",
            "| ner             | ontonotes |\n",
            "| forward_charlm  | 1billion  |\n",
            "| backward_charlm | 1billion  |\n",
            "===============================\n",
            "\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.2.0/en/tokenize/combined.pt: 100%|██████████| 640k/640k [00:00<00:00, 2.26MB/s]\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.2.0/en/ner/ontonotes.pt: 100%|██████████| 166M/166M [00:27<00:00, 6.01MB/s]\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.2.0/en/forward_charlm/1billion.pt: 100%|██████████| 22.7M/22.7M [00:00<00:00, 23.5MB/s]\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.2.0/en/backward_charlm/1billion.pt: 100%|██████████| 22.7M/22.7M [00:00<00:00, 23.3MB/s]\n",
            "2022-04-22 07:30:44 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "2022-04-22 07:30:44 INFO: Loading these models for language: en (English):\n",
            "=========================\n",
            "| Processor | Package   |\n",
            "-------------------------\n",
            "| tokenize  | combined  |\n",
            "| ner       | ontonotes |\n",
            "=========================\n",
            "\n",
            "2022-04-22 07:30:44 INFO: Use device: cpu\n",
            "2022-04-22 07:30:44 INFO: Loading: tokenize\n",
            "2022-04-22 07:30:44 INFO: Loading: ner\n",
            "2022-04-22 07:30:45 INFO: Done loading processors!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# not done: 12, 17, 38, 39\n",
        "\n",
        "ls_num = 16\n",
        "li = [181]\n",
        "for file in li:\n",
        "  stanza_documents: List[Document] = []\n",
        "  text_filename = '/content/drive/MyDrive/Lok Sabha Data/textfiles/' + str(ls_num) + '/' + str(file) + '.csv'\n",
        "  textfile = pd.read_csv(text_filename, lineterminator='\\n', names=['index','speech'])\n",
        "  if textfile['speech'].isnull().all():\n",
        "    print(str(ls_num) + '-' + str(file) + ' is empty!')\n",
        "  else:\n",
        "    speech = [x for x in textfile['speech'].values if x]\n",
        "    for document in batch(speech, nlp, batch_size=32):\n",
        "      stanza_documents.append(document)\n",
        "    ner = []\n",
        "    for i in range(len(stanza_documents)):\n",
        "      for j in range(len(stanza_documents[i].sentences)):\n",
        "        for k in range(len(stanza_documents[i].sentences[j].ents)):\n",
        "          ner.append([i-1, j, k, stanza_documents[i].sentences[j].ents[k].text, stanza_documents[i].sentences[j].ents[k].type]) \n",
        "    NER = pd.DataFrame(ner, columns=['speech#', 'sent#', 'ent#', 'entity_text', 'entity_type'])\n",
        "    NER_filename = '/content/drive/MyDrive/Lok Sabha Data1/NER/' + str(ls_num) + '/' + str(file) + '_NER.csv'\n",
        "    NER.to_csv(NER_filename)\n",
        "    print(str(ls_num) + '-' + str(file) + ' NER done!')"
      ],
      "metadata": {
        "id": "TTMA3k7I3k0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFPNREVq38LL",
        "outputId": "28e49117-d1ea-42bf-e2d2-f90716e3d081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stanza for Hindi POS-tagging"
      ],
      "metadata": {
        "id": "vxqcl-_d4DHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "import pandas as pd\n",
        "from stanza.models.common.doc import Document\n",
        "from typing import List\n",
        "from stanza_batch import batch\n",
        "stanza.download(lang='hi', processors='tokenize,pos')\n",
        "nlp = stanza.Pipeline(lang='hi', processors='tokenize,pos')\n"
      ],
      "metadata": {
        "id": "_gj2OmC64Zac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls_num = 17\n",
        "li = list(range(112,1,-1))\n",
        "for file in li:\n",
        "  stanza_documents: List[Document] = []\n",
        "  text_filename = '/content/drive/MyDrive/Lok Sabha Data/textfiles/' + str(ls_num) + '/' + str(file) + '.csv'\n",
        "  textfile = pd.read_csv(text_filename, lineterminator='\\n', names=['index','speech'])\n",
        "  if textfile['speech'].isnull().all():\n",
        "    print(str(ls_num) + '-' + str(file) + ' is empty!')\n",
        "  else:\n",
        "    speech = [x for x in textfile['speech'].values if x]\n",
        "    for document in batch(speech, nlp, batch_size=32):\n",
        "      stanza_documents.append(document)\n",
        "    pos = []\n",
        "    for i in range(len(stanza_documents)):\n",
        "      for j in range(len(stanza_documents[i].sentences)):\n",
        "        for k in range(len(stanza_documents[i].sentences[j].words)):\n",
        "          pos.append([i-1, j, k, stanza_documents[i].sentences[j].words[k].text, stanza_documents[i].sentences[j].words[k].upos, stanza_documents[i].sentences[j].words[k].xpos, stanza_documents[i].sentences[j].words[k].feats]) \n",
        "    POS = pd.DataFrame(pos, columns=['speech#', 'sent#', 'word#', 'word', 'word_upos', 'word_xpos', 'word_feats'])\n",
        "    POS_filename = '/content/drive/MyDrive/Lok Sabha Data/POS/' + str(ls_num) + '/' + str(file) + '_POS.csv'\n",
        "    POS.to_csv(POS_filename)\n",
        "    print(str(ls_num) + '-' + str(file) + ' POS done!')"
      ],
      "metadata": {
        "id": "goYzUGuR4dgh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}